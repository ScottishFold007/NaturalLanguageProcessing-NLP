{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author: Kevin Okiah\n",
    "## NLP :  Lexical Density Analysis and Vocabulary Count.\n",
    "## MSDS\n",
    "## 01/10/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a teaser to Natural Language Processing (NLP). It explores calculation of Lexical density and Vocabulary Count.\n",
    "\n",
    "Lexical analysis is the lowest level of NLP. See figure 1 below for the levels of NLP. Lexical analysis involves identifying and analyzing the structure of words.\n",
    "\n",
    "\n",
    "![](levels_of_nlp.JPG)\n",
    "\n",
    "                                              \n",
    "                                                   Fig 1. Levels of NLP\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook a download 3 graded readers books from http://www.gutenberg.org/wiki/Children%27s_Instructional_Books_(Bookshelf) and leverage NLTK library to preprocess the books by:\n",
    "\n",
    "1. Remove Guternberg headers and footers\n",
    "2. Parse the texts breaking the books into tokens\n",
    "3. Expand contractions\n",
    "4. Remove stop words, punctuations and Alpha numeric\n",
    "5. stem and lemmatize the tokens\n",
    "\n",
    "I then proceed to perform lexical density and Vocabulary analysis on the books. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\tInstall Python (if you don’t have it already), and install NLTK.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: NLTK in /home/kevimwe/anaconda3/envs/py36/lib/python3.6/site-packages (3.4)\n",
      "Requirement already satisfied: singledispatch in /home/kevimwe/anaconda3/envs/py36/lib/python3.6/site-packages (from NLTK) (3.4.0.3)\n",
      "Requirement already satisfied: six in /home/kevimwe/anaconda3/envs/py36/lib/python3.6/site-packages (from NLTK) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Follow the instructions in chapter 1 of Bird-Klein for implementing a “lexical diversity” scoring routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library Imports\n",
    "import nltk\n",
    "from urllib import request\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "        return len(set(text)) / len(text) \n",
    "def percentage(count, total):\n",
    "    return 100 * count / total\n",
    "def FreqDistPlot(data, show =10):\n",
    "    fdist1 = FreqDist(data) \n",
    "    fdist1.plot(show, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\tGo to http://www.gutenberg.org/wiki/Children%27s_Instructional_Books_(Bookshelf), and obtain three texts (of different grade levels) from the “Graded Readers” section. Report the lexical diversity score of each. Explain whether the result was surprising.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {\"2nd\":\"http://www.gutenberg.org/cache/epub/15659/pg15659.txt\", #2nd grader book\n",
    "        \"4th\":\"http://www.gutenberg.org/cache/epub/18702/pg18702.txt\", #4th grader book\n",
    "        \"HS\":\"http://www.gutenberg.org/cache/epub/19923/pg19923.txt\"}  #High School grader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick look at the lexical_diversity scores on raw uncleaned Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Score(book ='2nd'):\n",
    "    url =urls[book]\n",
    "    respose = request.urlopen(url)\n",
    "    raw = respose.read().decode('utf8')\n",
    "    return lexical_diversity(raw.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2515451943418924"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Score('2nd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22280725890165964"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Score('4th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20531262962644106"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Score('HS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we clean the data removing gutenberg headers sections, footnotes sections, contents sections, stopwords  and stem the tokens for the book before running the lexical assessment. All the tokens are converted to lower to avoid double counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this are markers for the first and last lines for the books to filter only the needed sections\n",
    "FirstLine = {'2nd': \"[Illustration]\", \n",
    "             '4th': 'The Good Land',\n",
    "              'HS':'INTRODUCTORY.'}\n",
    "LastLine  = {'2nd': \"End of Project Gutenberg's The Beacon Second Reader, by James H. Fassett\",\n",
    "             '4th': \"***END OF THE PROJECT GUTENBERG EBOOK THE ONTARIO READERS: FOURTH BOOK***\",\n",
    "              'HS':'End of the Project Gutenberg EBook of The Ontario Readers: The High School'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code adopted from text Text-analytics-with-python-a-practical-dipanjan-sarkar\n",
    "def remove_stopwords(tokens): \n",
    "    stopword_list = nltk.corpus.stopwords.words('english') \n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list] \n",
    "    return filtered_tokens \n",
    "\n",
    "#this function stems the tokens to get the root\n",
    "def stemming(tokens):\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_list =[]\n",
    "    for i in tokens:\n",
    "        stemmed_list = stemmed_list+[ps.stem(i)]\n",
    "    return stemmed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function reads in books url, filters out this Sections: Content, preface and gutenberg header and footer.\n",
    "The funtion then splits the data into tokens for analysis.\n",
    "'''\n",
    "\n",
    "def CleanBookTokens(book ='HS'):\n",
    "    url =urls[book]\n",
    "    respose = request.urlopen(url)\n",
    "    raw = respose.read().decode('utf8')\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"-\", \"\") # don't remove hyphens\n",
    "    pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "    Temp = list(raw.splitlines()) #Split raw the test in raw and store in an array\n",
    "    flag = 0 #flag to track when to read and close a book\n",
    "    CleanBook = [] #array to hold the desired sentences\n",
    "    CleanTokens = []\n",
    "    Tokens =[]\n",
    "    for i in Temp:\n",
    "        if(i==FirstLine[book]):\n",
    "            flag =1\n",
    "        if(i==LastLine[book]):\n",
    "            flag =2\n",
    "        if(flag==1):\n",
    "            CleanBook.append(i)\n",
    "            Tokens = Tokens + re.sub(pattern, \"\", i.lower()).split() #tokens to lower\n",
    "            #print(i)\n",
    "    CleanTokens = remove_stopwords(Tokens)  # Remove stopwords\n",
    "    CleanTokens = stemming(CleanTokens) #stem the tokens to get root words\n",
    "    return CleanTokens, CleanBook\n",
    "\n",
    "\n",
    "HighSchoolerReaderTokens,HighSchoolerReaderBook = CleanBookTokens('HS')\n",
    "SecondReaderTokens, SecondReaderBook = CleanBookTokens('2nd')\n",
    "FourthReaderTokens, FourthReaderBook = CleanBookTokens('4th')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical diverisity for the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2477844735909252"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_diversity(SecondReaderTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19038230097559727"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_diversity(FourthReaderTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16669608897305452"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_diversity(HighSchoolerReaderTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above are not suprising.  It is expected that the Lexical Diversity increases as children progresses with grade level. For 2nd grade,  you would expect to have alot more text repetation thus a lower Lexical Diversity compared to high school text which would have a higher Lexical Diversity as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\tAlso compare the vocabulary size of the same three texts. Explain whether the result was surprising.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2796"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(SecondReaderTokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7435"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(FourthReaderTokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10387"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(HighSchoolerReaderTokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above are not suprising. Children's Vocabulary size increases as they progress with grade levels. It is expected that a child at grades 2 would learn less Vocabulary than a student at grade 4 whose vocabulary size would potentially have doubled. On an average student is likely to learn approximately 3,000 new vocabulary words each year. \n",
    "\n",
    "**Reference:** http://www.ascd.org/publications/books/113040/chapters/What-Does-the-Research-Say-About-Vocabulary%C2%A2.aspx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\tWrite a paragraph arguing whether vocabulary size and lexical diversity in combination could be a better measure of text difficulty (or reading level) than either measure is by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using them both would give a better measure of text difficulty. Lexical Diversity refers to “the range of different words used in a text, with a greater range indicating a higher diversity but is is pretty sensitive to text length. In our comparison above Lexical diversity might not be that effective as a measure as the three books are of different sizes thus using vocabulary size as a measure could help suppliment the assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
